from fastapi import FastAPI, Query
from collections import Counter
from fastapi.middleware.cors import CORSMiddleware
from typing import List, Optional
from data_fetcher import StockDataFetcher
from sentiment_analyzer import SentimentAnalyzer
from strategy_engine import StrategyEngine
from database import DatabaseManager
from industry_matcher import IndustryMatcher
from news_time_decay import NewsTimeDecay
from backtest_evaluator import BacktestEvaluator
from one_night_strategy import OneNightStrategy
import datetime
import asyncio
import threading
import time
import concurrent.futures
from concurrent.futures import ThreadPoolExecutor
import json
import numpy as np
import pandas as pd

app = FastAPI(title="StockSearch API")
fetcher = StockDataFetcher()
analyzer = SentimentAnalyzer()
engine = StrategyEngine()
db = DatabaseManager()
industry_matcher = IndustryMatcher()
# åˆå§‹åŒ–æ—¶é—´è¡°å‡å¤„ç†å™¨ï¼šæ–°é—»ä¿ç•™3å¤©ï¼Œè€ƒè™‘äº¤æ˜“æ—¥
time_decay = NewsTimeDecay(max_age_days=3, trading_days_only=True)
# åˆå§‹åŒ–å›æµ‹è¯„ä¼°å™¨ï¼šæ­¢ç›ˆ+10%ï¼Œæ­¢æŸ-5%ï¼Œæœ€å¤§æŒä»“30å¤©
backtest_evaluator = BacktestEvaluator(
    take_profit=0.10,
    stop_loss=-0.05,
    max_hold_days=30,
    fixed_periods=[5, 10, 20, 30]
)
one_night_strategy = OneNightStrategy(fetcher, db)

# å…è®¸è·¨åŸŸè¯·æ±‚
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

class NewsScanner:
    """ç‹¬ç«‹çš„æ–°é—»å¿«è®¯æ‰«æå¼•æ“ï¼šå…¨å¤©å€™è¿è¡Œ (æ¯ 5 åˆ†é’Ÿä¸€æ¬¡)"""
    def __init__(self, db, analyzer):
        self.db = db
        self.analyzer = analyzer
        self.is_running = True
        self.last_cleanup_time = time.time()
        self.cleanup_interval = 86400  # æ¯24å°æ—¶æ¸…ç†ä¸€æ¬¡è¿‡æœŸæ–°é—»

    def scan_loop(self):
        print("[NewsScanner] Background News Scanner Thread Started.")
        while self.is_running:
            try:
                # 1. æ‹‰å–å…¨å±€å¿«è®¯ (è€ƒè™‘åˆ° Railway æµ·å¤– IP è®¿é—®é™åˆ¶ï¼Œå°è¯•å¤šä¸ªæº)
                import akshare as ak
                # print("[NewsScanner] Fetching latest global news...")
                news_df = None
                
                # å°è¯•æº 1: å…¨çƒå¿«è®¯
                try:
                    news_df = ak.stock_info_global_cls()
                except Exception as e:
                    print(f"[NewsScanner] Source 1 (global_cls) failed: {e}")

                # å°è¯•æº 2: è´¢è”ç¤¾ç”µæŠ¥ (ä½œä¸ºå¤‡é€‰)
                if news_df is None or news_df.empty:
                    try:
                        print("[NewsScanner] Source 1 empty, trying Source 2 (stock_telegraph_cls)...")
                        news_df = ak.stock_telegraph_cls()
                    except Exception as e:
                        print(f"[NewsScanner] Source 2 (telegraph_cls) failed: {e}")
                
                # Check 
                if news_df is None or news_df.empty:
                    print("[NewsScanner] No news fetched via any akshare sources. Retrying in 5 mins.")
                    time.sleep(300)
                    continue

                print(f"[NewsScanner] Successfully fetched {len(news_df)} news items. Starting NLP analysis...")
                news_list_raw = news_df.head(30).to_dict(orient="records") # å‡å°‘å•æ¬¡åˆ†ææ•°é‡ï¼Œæé«˜å“åº”é€Ÿåº¦
                
                processed_news_input = []
                for n in news_list_raw:
                    content = n.get("å†…å®¹") or n.get("content") or ""
                    publish_time = n.get("å‘å¸ƒæ—¶é—´") or n.get("time") or ""
                    title = n.get("æ ‡é¢˜") or n.get("title") or ""
                    if content:
                        processed_news_input.append({
                            "å†…å®¹": content,
                            "å‘å¸ƒæ—¶é—´": publish_time,
                            "æ ‡é¢˜": title
                        })
                
                # 2. è°ƒç”¨ Analyzer è¿›è¡Œå¹¶å‘åˆ†æ (æ”¯æŒ LLM)
                news_pool = self.analyzer.batch_analyze(processed_news_input)
                
                # 3. æŒä¹…åŒ–åˆ°æ•°æ®åº“
                # ä¿®æ­£ï¼šRailway å®¹å™¨é€šå¸¸æ˜¯ UTC æ—¶é—´ï¼Œéœ€è½¬æ¢ä¸ºåŒ—äº¬æ—¶é—´ï¼ˆUTC+8ï¼‰
                utc_now = datetime.datetime.now(datetime.timezone.utc)
                beijing_now = utc_now + datetime.timedelta(hours=8)
                current_date = beijing_now.date()
                
                news_to_save = []
                for item in news_pool:
                    publish_time = item.get("å‘å¸ƒæ—¶é—´", "")
                    # å¦‚æœæ—¶é—´åªæœ‰æ—¶é—´æ²¡æœ‰æ—¥æœŸï¼Œæ·»åŠ å½“å‰æ—¥æœŸ
                    if publish_time:
                        publish_time_str = str(publish_time)
                        if ':' in publish_time_str and len(publish_time_str.split(':')) >= 2:
                            if len(publish_time_str) <= 8 and '-' not in publish_time_str:
                                # åªæœ‰æ—¶é—´ï¼Œæ·»åŠ å½“å‰æ—¥æœŸ
                                publish_time = f"{current_date} {publish_time_str}"
                    
                    news_to_save.append({
                        "title": item.get("æ ‡é¢˜"),
                        "content": item.get("å†…å®¹"),
                        "time": publish_time,
                        "sentiment": item.get("sentiment")
                    })
                
                if news_to_save:
                    self.db.save_news_batch(news_to_save)
                    print(f"[NewsScanner] {len(news_to_save)} news items synced to DB at {beijing_now.strftime('%H:%M:%S')} (CN Time).")
                
                # å®šæœŸæ¸…ç†è¿‡æœŸæ–°é—»ï¼ˆæ¯24å°æ—¶ä¸€æ¬¡ï¼‰
                current_time = time.time()
                if current_time - self.last_cleanup_time > self.cleanup_interval:
                    try:
                        deleted_count = self.db.cleanup_old_news(max_age_days=7)
                        if deleted_count > 0:
                            print(f"[NewsScanner] Cleaned up {deleted_count} old news items.")
                        self.last_cleanup_time = current_time
                    except Exception as e:
                        print(f"[NewsScanner] Error during cleanup: {e}")
                
                # æ¯ 5 åˆ†é’Ÿè½®è¯¢ä¸€æ¬¡
                time.sleep(300)
            except Exception as e:
                print(f"[NewsScanner] Error in loop: {e}")
                time.sleep(60)

class StrategyScheduler:
    """ç­–ç•¥å®šæ—¶è°ƒåº¦å™¨"""
    def __init__(self, strategy):
        self.strategy = strategy
        self.is_running = True
        self.last_buy_date = None
        self.last_sell_date = None

    def run_loop(self):
        print("[Scheduler] Strategy Scheduler Thread Started.")
        while self.is_running:
            try:
                now = datetime.datetime.now() # Server local time
                # ä¿®æ­£ï¼šRailway å®¹å™¨é€šå¸¸æ˜¯ UTC æ—¶é—´ï¼Œéœ€è½¬æ¢ä¸ºåŒ—äº¬æ—¶é—´ï¼ˆUTC+8ï¼‰
                # æ³¨æ„ï¼šdockerå®¹å™¨é‡Œ datetime.now() å¯èƒ½æ˜¯ UTC
                # æœ€å¥½ç»Ÿä¸€ç”¨ UTC+8 åˆ¤æ–­
                # ä½¿ç”¨æ—¶åŒºæ„ŸçŸ¥çš„ UTC æ—¶é—´ï¼ˆé¿å… DeprecationWarningï¼‰
                utc_now = datetime.datetime.now(datetime.timezone.utc)
                beijing_now = utc_now + datetime.timedelta(hours=8)
                
                current_date_str = beijing_now.strftime("%Y-%m-%d")
                current_time_str = beijing_now.strftime("%H:%M")
                
                # Check Sell (09:40)
                if current_time_str == "09:40" and self.last_sell_date != current_date_str:
                    print(f"[Scheduler] Triggering Daily Sell Routine at {current_time_str}...")
                    self.strategy.daily_sell_routine()
                    self.last_sell_date = current_date_str
                
                # Check Buy (14:30)
                if current_time_str == "14:30" and self.last_buy_date != current_date_str:
                    print(f"[Scheduler] Triggering Daily Buy Routine at {current_time_str}...")
                    self.strategy.daily_buy_routine()
                    self.last_buy_date = current_date_str
                
                time.sleep(30) # Check every 30s
            except Exception as e:
                print(f"[Scheduler] Error: {e}")
                time.sleep(60)

class BackgroundScanner:
    """åå°å¼‚æ­¥æ‰«æå¼•æ“ï¼šè´Ÿè´£å…¨å¸‚åœºè‡ªåŠ¨å¯»è¿¹"""
    def __init__(self, fetcher, engine, db):
        self.fetcher = fetcher
        self.engine = engine
        self.db = db
        self.latest_results = []
        self.is_running = True
        self.scan_count = 0
        self.last_scan_date = "" 
        self.reset_event = threading.Event()

        # å¯åŠ¨è‡ªæ£€ï¼šå°è¯•ä»æ•°æ®åº“æ¢å¤ä»Šæ—¥å·²æœ‰çš„æ‰«æå¿«ç…§
        today_str = datetime.date.today().strftime("%Y-%m-%d")
        historical_results = self.db.get_daily_scan(today_str)
        if historical_results:
            print(f"[Scanner] Startup: Restored {len(historical_results)} results for {today_str} from database.")
            self.latest_results = historical_results
            self.last_scan_date = today_str

    def trigger_scan(self):
        """æ‰‹åŠ¨å¤–éƒ¨å”¤é†’æ‰«æ (å¦‚æƒé‡å˜æ›´å)ï¼Œé‡ç½®æ—¥æœŸå¼ºåˆ¶é‡æ‰«"""
        print("[Scanner] Signal received: Resetting date and triggering immediate FULL-MARKET re-scan...")
        self.last_scan_date = "" 
        self.reset_event.set()

    def scan_loop(self):
        print("[Scanner] Background Full-Market Scanner Thread Started.")
        while self.is_running:
            try:
                # ä¿®æ­£ï¼šRailway å®¹å™¨é€šå¸¸æ˜¯ UTC æ—¶é—´ï¼Œéœ€è½¬æ¢ä¸ºåŒ—äº¬æ—¶é—´ï¼ˆUTC+8ï¼‰
                utc_now = datetime.datetime.now(datetime.timezone.utc)
                beijing_now = utc_now + datetime.timedelta(hours=8)
                today_str = beijing_now.strftime("%Y-%m-%d")
                    
                # å¦‚æœä»Šå¤©å·²ç»æ‰«è¿‡äº†ï¼Œä¸”æ²¡æœ‰æ”¶åˆ°å¼ºåˆ¶é‡æ‰«ä¿¡å·ï¼Œå°±è¿›å…¥é•¿ä¼‘çœ 
                if self.last_scan_date == today_str:
                    print(f"[Scanner] Today's scan ({today_str}) already complete. Waiting for reset or next day...")
                    if self.reset_event.wait(3600): # æ¯å°æ—¶æ£€æŸ¥ä¸€æ¬¡ï¼Œé™¤éè¢« reset_event å”¤é†’
                         print("[Scanner] Manual trigger detected. Restarting scan...")
                    self.reset_event.clear()
                    continue
    
                # ä¿®æ­£ï¼šè·¨å¤©åå¿…é¡»ç­‰åˆ°æ”¶ç›˜å (15:00) æ‰èƒ½è·å–å½“æ—¥å…¨é‡æ•°æ®
                if beijing_now.hour < 15 and not self.reset_event.is_set():
                    print(f"[Scanner] It's {beijing_now.strftime('%H:%M')} (CN Time), market not closed yet. Waiting for 15:00...")
                    if self.reset_event.wait(1800): # æ¯ 30 åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡
                        print("[Scanner] Manual trigger detected. Force starting scan...")
                    else:
                        continue
    
                print(f"\n[Scanner] === Starting FULL-MARKET Scan #{self.scan_count} ({today_str}) ===")
                
                # ğŸ”„ é˜¶æ®µ 1ï¼šå…ˆæ‰§è¡Œå…¨é‡ K çº¿ç¼“å­˜ï¼ˆä¸ºåç»­ç­–ç•¥æä¾›æ•°æ®åŸºç¡€ï¼‰
                print("[Scanner] Phase 1: Pre-caching K-line data for active stocks...")
                stock_list = self.fetcher.get_all_stocks()
                if stock_list.empty:
                    print("[Scanner] ERROR: Cannot fetch stock list.")
                    time.sleep(300)
                    continue
                
                # ak.stock_info_a_code_name() è¿”å›çš„åˆ—åæ˜¯ 'code', 'name' (è‹±æ–‡)
                symbols = stock_list['code'].tolist()
                print(f"[Scanner] Total {len(symbols)} stocks to cache.")
                
                # ä½¿ç”¨ 3 çº¿ç¨‹å¹¶å‘ç¼“å­˜ï¼ˆä¸åšåˆ†æï¼Œåªå­˜ K çº¿ï¼‰
                from concurrent.futures import ThreadPoolExecutor
                import random
                
                def cache_stock_kline(symbol):
                    try:
                        time.sleep(0.1 + random.random() * 0.2)  # é˜²å°å»¶è¿Ÿ
                        cached = self.db.get_cached_kline(symbol, max_age_hours=48)  # 2å¤©å†…æœ‰æ•ˆ
                        if cached is None or cached.empty:
                            kline = self.fetcher.get_kline_data(symbol, days=30)  # åªéœ€ 30 å¤©æ•°æ®
                            if kline is not None and not kline.empty:
                                self.db.save_kline(symbol, kline)
                        return True
                    except:
                        return False
                
                cached_count = 0
                with ThreadPoolExecutor(max_workers=3) as executor:
                    results = list(executor.map(cache_stock_kline, symbols))
                    cached_count = sum(results)
                
                print(f"[Scanner] Phase 1 Complete: {cached_count}/{len(symbols)} stocks cached.")
                
                # ğŸ¯ é˜¶æ®µ 2ï¼šæ‰§è¡Œâ€œä¸€å¤œæŒè‚¡â€ç­–ç•¥ç­›é€‰ï¼ˆå®Œå…¨ä¾èµ–ç¼“å­˜ï¼‰
                print("[Scanner] Phase 2: Running OneNight strategy with cached data...")
                new_recommendations = one_night_strategy.scan_market()
                
                if not new_recommendations:
                    print("[Scanner] Warning: Strategy scan returned empty list.")
                    self.latest_results = []
                else:
                    # æŒ‰åˆ†æ•° (é‡æ¯”) æ’åº
                    full_ranks = sorted(new_recommendations, key=lambda x: x.get('score', 0), reverse=True)
                    self.latest_results = full_ranks[:12] # ä¸»é¡µæ˜¾ç¤º Top 12
                    self.last_scan_date = today_str 
                    
                    # å°†å…¨éƒ¨ç»“æœå­˜å…¥æ•°æ®åº“ä¾›æ’è¡Œæ¦œåˆ†é¡µæŸ¥é˜…
                    self.db.save_daily_scan(today_str, full_ranks)
                    print(f"[Scanner] Full Scan Complete. Saved {len(full_ranks)} stocks to Database for Market Ranking.")
                
                self.scan_count += 1
                self.reset_event.clear()
            except Exception as e:
                print(f"[Scanner] Critical Error in Loop: {e}")
                time.sleep(60)

# åˆå§‹åŒ–å¹¶å¯åŠ¨åå°æ‰«æå¼•æ“
scanner = BackgroundScanner(fetcher, engine, db)
threading.Thread(target=scanner.scan_loop, daemon=True).start()

# å¯åŠ¨ç‹¬ç«‹çš„æ–°é—»æ‰«æå¼•æ“
news_scanner = NewsScanner(db, analyzer)
threading.Thread(target=news_scanner.scan_loop, daemon=True).start()

# å¯åŠ¨ç­–ç•¥è°ƒåº¦å™¨
strategy_scheduler = StrategyScheduler(one_night_strategy)
threading.Thread(target=strategy_scheduler.run_loop, daemon=True).start()

@app.get("/")
async def root():
    return {"message": "StockSearch API is running"}

@app.get("/api/health")
async def health_check():
    return {"status": "healthy"}

@app.get("/api/stocks/realtime")
def get_realtime_stocks(symbols: Optional[str] = Query(None)):
    """è·å–å®æ—¶è¡Œæƒ…ï¼Œsymbols ä»¥é€—å·åˆ†éš”"""
    symbol_list = symbols.split(",") if symbols else []
    df = fetcher.get_realtime_quotes(symbol_list)
    return df.to_dict(orient="records")

@app.get("/api/stocks/kline/{symbol}")
def get_stock_kline(symbol: str, period: str = "daily", days: int = 200):
    """è·å– K çº¿å†å²æ•°æ®"""
    start_date = (datetime.datetime.now() - datetime.timedelta(days=days)).strftime("%Y%m%d")
    df = fetcher.get_kline_data(symbol, period=period, start_date=start_date)
    return df.to_dict(orient="records")

@app.get("/api/stocks/info/{symbol}")
def get_stock_info(symbol: str):
    """è·å–ä¸ªè‚¡åŸºæœ¬é¢åŸºç¡€ä¿¡æ¯ (åç§°ã€è¡Œä¸šç­‰)"""
    return fetcher.get_stock_info(symbol)

@app.get("/api/stocks/finance/{symbol}")
def get_stock_finance(symbol: str):
    """è·å–å…¬å¸æ ¸å¿ƒè´¢åŠ¡æŒ‡æ ‡"""
    return fetcher.get_company_finance(symbol)

@app.get("/api/industry/sentiment")
def get_industry_sentiment():
    """è·å–è¡Œä¸šæƒ…ç»ªå€¼ï¼ˆå½“å‰å‘¨å’Œä¸Šå‘¨ï¼‰"""
    try:
        # å¦‚æœå½“å‰å‘¨æ²¡æœ‰æ•°æ®ï¼Œå°è¯•ä»å†å²æ–°é—»åˆå§‹åŒ–
        week_start = db.get_week_start_date()
        conn = db.get_connection()
        try:
            cursor = conn.cursor()
            cursor.execute('''
                SELECT COUNT(*) FROM industry_sentiment_weekly 
                WHERE week_start_date = ?
            ''', (week_start,))
            count = cursor.fetchone()[0]
            if count == 0:
                # å½“å‰å‘¨æ²¡æœ‰æ•°æ®ï¼Œä»å†å²æ–°é—»åˆå§‹åŒ–
                print(f"[API] No sentiment data for current week ({week_start}). Initializing from history...")
                init_count = db.initialize_industry_sentiment_from_history()
                print(f"[API] Initialized {init_count} industry sentiment records.")
        finally:
            conn.close()
        
        result = db.get_current_and_last_week_sentiment()
        print(f"[API] Returning sentiment data: current_week={len(result.get('current_week', []))}, last_week={len(result.get('last_week', []))}")
        return result
    except Exception as e:
        print(f"[API] Error getting industry sentiment: {e}")
        import traceback
        traceback.print_exc()
        # å³ä½¿å‡ºé”™ä¹Ÿè¿”å›ä¸€ä¸ªç©ºç»“æ„ï¼Œè®©å‰ç«¯èƒ½æ­£å¸¸æ˜¾ç¤º
        return {
            "current_week": [],
            "last_week": [],
            "current_week_start": "",
            "last_week_start": "",
            "market_sentiment": 1.0,
            "error": str(e)
        }

@app.get("/api/news/flash")
def get_news_flash():
    """è·å–æœ€æ–°è´¢ç»å¿«è®¯ (ä¼˜å…ˆè¯»åº“ï¼Œå®ç°æ¯«ç§’çº§å“åº”)"""
    try:
        # 1. å°è¯•ä»æ•°æ®åº“è¯»å–ç°æˆç»“æœ
        cached_news = db.get_latest_news(limit=20)
        if cached_news:
            return cached_news
            
        # 2. å¦‚æœåº“ä¸ºç©º (æ¯”å¦‚åˆšå¯åŠ¨è¿˜æ²¡è¿è¡Œ scanner)ï¼Œåˆ™é™çº§ä¸ºå®æ—¶æ‹‰å–
        # æ³¨æ„ï¼šè¿™ä¼šæ¯”è¾ƒæ…¢ï¼Œå› ä¸ºåŒ…å« LLM è°ƒç”¨
        import akshare as ak
        import pandas as pd
        news_df = ak.stock_info_global_cls()
        news_df = news_df.rename(columns={
            "å‘å¸ƒæ—¶é—´": "time",
            "å†…å®¹": "content",
            "æ ‡é¢˜": "title"
        })
        # ç¡®ä¿æŒ‰æ—¶é—´é™åºæ’åºï¼ˆæœ€æ–°çš„åœ¨å‰ï¼‰
        if "time" in news_df.columns and not news_df.empty:
            # å°è¯•å°†æ—¶é—´è½¬æ¢ä¸º datetime è¿›è¡Œæ’åº
            try:
                current_date = datetime.datetime.now().date()
                # å¦‚æœæ—¶é—´åªæœ‰æ—¶é—´æ²¡æœ‰æ—¥æœŸï¼Œæ·»åŠ å½“å‰æ—¥æœŸ
                def add_date_if_needed(time_str):
                    if pd.isna(time_str) or not time_str:
                        return None
                    time_str = str(time_str)
                    # å¦‚æœåªæœ‰æ—¶é—´ï¼ˆé•¿åº¦ <= 8ï¼Œå¦‚ "14:30:00"ï¼‰ï¼Œæ·»åŠ å½“å‰æ—¥æœŸ
                    if ':' in time_str and len(time_str) <= 8 and '-' not in time_str:
                        return f"{current_date} {time_str}"
                    return time_str
                
                news_df['time'] = news_df['time'].apply(add_date_if_needed)
                news_df['time'] = pd.to_datetime(news_df['time'], errors='coerce')
                news_df = news_df.sort_values('time', ascending=False)
                # è½¬æ¢å›å­—ç¬¦ä¸²æ ¼å¼ä»¥ä¾¿è¿”å›ï¼ˆåŒ…å«å®Œæ•´æ—¥æœŸæ—¶é—´ï¼‰
                news_df['time'] = news_df['time'].dt.strftime('%Y-%m-%d %H:%M:%S')
            except Exception as e:
                print(f"[News] Error processing time: {e}")
                # å¦‚æœè½¬æ¢å¤±è´¥ï¼ŒæŒ‰å­—ç¬¦ä¸²é™åºæ’åº
                news_df = news_df.sort_values('time', ascending=False)
        
        news_list = news_df.head(20).to_dict(orient="records")
        processed_news = analyzer.batch_analyze(news_list)
        
        # é¡ºä¾¿å­˜å…¥åº“
        db.save_news_batch(processed_news)
        
        # è¿”å›çš„åˆ—è¡¨å·²ç»æŒ‰æ—¶é—´é™åºæ’åºï¼ˆæœ€æ–°çš„åœ¨å‰ï¼‰
        return processed_news
    except Exception as e:
        print(f"News error: {e}")
        return {"error": str(e)}

@app.get("/api/stocks/recommend/{symbol}")
def get_stock_recommendation(symbol: str):
    """ã€å•è‚¡è¯Šæ–­ã€‘è·å–æŒ‡å®šä¸ªè‚¡çš„æ ¸å¿ƒç­–ç•¥å»ºè®®å¹¶æŒä¹…åŒ–è®°å½•"""
    try:
        kline_df = fetcher.get_kline_data(symbol, days=250)
        if kline_df.empty:
            return {"error": f"æ— æ³•è·å–è‚¡ç¥¨ {symbol} çš„æ•°æ®"}
            
        sentiment_score = 0.1 
        # è·å–å½“å‰åŠ¨æ€æƒé‡
        weights = db.get_weights()
        
        # è·å–è´¢åŠ¡æ•°æ®
        finance_data = None
        try:
            finance_list = fetcher.get_company_finance(symbol)
            if finance_list and len(finance_list) > 0:
                finance_data = finance_list[0]
        except:
            finance_data = None
        
        # è·å–å‘¨çº¿æ•°æ®
        weekly_kline = None
        try:
            weekly_kline = fetcher.get_kline_data(symbol, period="weekly", days=200)
            if weekly_kline.empty:
                weekly_kline = None
        except:
            weekly_kline = None
        
        recommendation = engine.generate_recommendation(
            kline_df, 
            sentiment_score,
            tech_weight=weights.get('tech_weight', 0.6),
            sentiment_weight=weights.get('sentiment_weight', 0.2),
            fundamental_weight=weights.get('fundamental_weight', 0.15),
            risk_weight=weights.get('risk_weight', 0.05),
            finance_data=finance_data,
            weekly_kline_df=weekly_kline
        )
        
        # è¡¥å…¨ä»£ç å­—æ®µ
        recommendation['symbol'] = symbol
        # å¼ºåˆ¶è½¬æ¢ä»·æ ¼ä¸º floatï¼Œé˜²æ­¢æ•°æ®åº“æŠ¥é”™
        recommendation['price'] = float(recommendation.get('price', 0))
        
        # è·å–è¡Œä¸šä¿¡æ¯
        stock_info = fetcher.get_stock_info(symbol)
        recommendation['industry'] = stock_info.get('è¡Œä¸š', 'æœªçŸ¥')
        
        # ä¿å­˜åˆ°æ•°æ®åº“ï¼ˆåªä¿å­˜BUYå’ŒHOLDï¼‰
        if recommendation['action'] in ["BUY", "HOLD"]:
            db.save_recommendation(recommendation)
        
        return recommendation
    except Exception as e:
        print(f"Error diagnosing {symbol}: {e}")
        return {"error": str(e)}

@app.get("/api/stocks/market_recommendations")
def get_market_recommendations():
    """ã€ä¸»é¡µçœ‹æ¿ã€‘è¿”å› Top 12 æ ¸å¿ƒæ¨è"""
    return scanner.latest_results[:12]

@app.get("/api/stocks/full_rank")
def get_full_market_rank(
    page: int = Query(1, ge=1), 
    page_size: int = Query(500, ge=1, le=1000),
    search: str = Query(None),
    industry: str = Query(None),
    min_price: float = Query(None),
    max_price: float = Query(None),
    sort_by: str = Query('score'),
    sort_dir: str = Query('desc')
):
    """ã€æ’è¡Œæ¦œé¡µã€‘è¿”å›å½“æ—¥å…¨é‡æ‰«æç»“æœï¼Œæ”¯æŒåˆ†é¡µã€æœç´¢ã€ç­›é€‰å’Œæ’åº"""
    today_str = datetime.date.today().strftime("%Y-%m-%d")
    # ä¼˜å…ˆä»æ•°æ®åº“åŠ è½½ä»Šæ—¥å®Œæ•´åå•
    ranks = db.get_daily_scan(today_str)
    if not ranks:
        # å¦‚æœæ•°æ®åº“è¿˜æ²¡å†™å®Œï¼Œå°è¯•ç”±å†…å­˜è¿”å›
        ranks = scanner.latest_results
    
    if not ranks:
        return {"data": [], "total": 0, "page": page, "page_size": page_size, "total_pages": 0}
    
    # 1. åº”ç”¨è¿‡æ»¤
    filtered_ranks = ranks
    
    # æœç´¢ (Symbol / Name)
    if search:
        search_lower = search.lower()
        filtered_ranks = [
            r for r in filtered_ranks 
            if search_lower in str(r.get('symbol', '')).lower() or search_lower in str(r.get('name', '')).lower()
        ]
        
    # è¡Œä¸šç­›é€‰
    if industry:
        filtered_ranks = [r for r in filtered_ranks if r.get('industry') == industry]
        
    # ä»·æ ¼åŒºé—´
    if min_price is not None:
        filtered_ranks = [r for r in filtered_ranks if float(r.get('price', 0)) >= min_price]
        
    if max_price is not None:
        filtered_ranks = [r for r in filtered_ranks if float(r.get('price', 0)) <= max_price]
        
    # 2. åº”ç”¨æ’åº
    reverse = (sort_dir == 'desc')
    try:
        # å¤„ç†å¯èƒ½ç¼ºå¤±çš„å­—æ®µï¼Œè®¾ç½®é»˜è®¤å€¼
        def get_sort_key(item):
            val = item.get(sort_by)
            if val is None:
                return -float('inf') if reverse else float('inf')
            try:
                return float(val) # å°è¯•è½¬ä¸ºæ•°å­—
            except (ValueError, TypeError):
                return str(val) # å¦åˆ™æŒ‰å­—ç¬¦ä¸²æ’åº

        filtered_ranks.sort(key=get_sort_key, reverse=reverse)
    except Exception as e:
        print(f"Sort error: {e}")
        # Fallback to score sort if custom sort fails
        filtered_ranks.sort(key=lambda x: x.get('score', 0), reverse=True)
    
    # 3. è®¡ç®—åˆ†é¡µ
    total = len(filtered_ranks)
    total_pages = (total + page_size - 1) // page_size if total > 0 else 0
    start_idx = (page - 1) * page_size
    end_idx = min(start_idx + page_size, total)
    
    # 4. è·å–å½“å‰é¡µæ•°æ®
    page_data = filtered_ranks[start_idx:end_idx]
    
    return {
        "data": page_data,
        "total": total,
        "page": page,
        "page_size": page_size,
        "total_pages": total_pages
    }

@app.get("/api/stocks/industry_distribution")
def get_industry_distribution(limit: int = Query(200, ge=10, le=2000)):
    """ç»Ÿè®¡å…¨åœºæ’åå‰ N åè‚¡ç¥¨çš„è¡Œä¸šåˆ†å¸ƒï¼ˆé¥¼å›¾æ•°æ®ï¼‰"""
    today_str = datetime.date.today().strftime("%Y-%m-%d")
    ranks = db.get_daily_scan(today_str)
    if not ranks:
        ranks = scanner.latest_results
        
    if not ranks:
        return []
        
    # æŒ‰åˆ†æ•°æ’åºç¡®ä¿æ˜¯ Top N
    sorted_stocks = sorted(ranks, key=lambda x: x.get('score', 0), reverse=True)
    top_stocks = sorted_stocks[:limit]
    
    # ç»Ÿè®¡
    industries = [s.get('industry', 'å…¶ä»–') for s in top_stocks if s.get('industry')]
    counter = Counter(industries)
    
    # å–å‰ 9 ä¸ªè¡Œä¸šï¼Œå…¶ä»–çš„å½’ä¸º "å…¶ä»–"
    top_industries = counter.most_common(9)
    other_count = len(top_stocks) - sum(item[1] for item in top_industries)
    
    result = [{"name": k, "value": v} for k, v in top_industries]
    if other_count > 0:
        result.append({"name": "å…¶ä»–", "value": other_count})
        
    return result

@app.get("/api/stocks/industries")
def get_all_industries():
    """è·å–å…¨å¸‚åœºæ‰€æœ‰å­˜åœ¨çš„è¡Œä¸šåˆ—è¡¨ï¼ˆç”¨äºç­›é€‰ï¼‰"""
    today_str = datetime.date.today().strftime("%Y-%m-%d")
    ranks = db.get_daily_scan(today_str)
    if not ranks:
        ranks = scanner.latest_results
        
    if not ranks:
        return []
        
    # æå–æ‰€æœ‰ä¸ä¸ºç©ºçš„è¡Œä¸šå¹¶å»é‡
    industries = sorted(list(set(r.get('industry') for r in ranks if r.get('industry'))))
    return industries

@app.post("/api/admin/trigger_scan")
def trigger_manual_scan():
    """ã€ç®¡ç†ç«¯ç‚¹ã€‘æ‰‹åŠ¨è§¦å‘å…¨é‡å¸‚åœºæ‰«æ"""
    try:
        # æ¸…é™¤ä»Šæ—¥æ‰«ææ ‡è®°ï¼Œå¼ºåˆ¶é‡æ–°æ‰«æ
        scanner.last_scan_date = None
        scanner.reset_event.set()
        return {"status": "success", "message": "Full market scan triggered. Please wait 5-10 minutes for completion."}
    except Exception as e:
        return {"status": "error", "message": str(e)}

@app.get("/api/strategy/onenight/status")
def get_onenight_status():
    """è·å–ä¸€å¤œæŒè‚¡ç­–ç•¥å½“å‰çŠ¶æ€ (æŒä»“)"""
    active_trades = db.get_active_trades()
    return active_trades.to_dict(orient="records")

@app.get("/api/strategy/onenight/history")
def get_onenight_history():
    """è·å–ä¸€å¤œæŒè‚¡ç­–ç•¥å†å²æˆ˜ç»©"""
    history = db.get_trade_history(limit=100)
    return history.to_dict(orient="records")

@app.post("/api/strategy/onenight/trigger_buy")
def trigger_onenight_buy():
    """ã€æµ‹è¯•ç”¨ã€‘æ‰‹åŠ¨è§¦å‘ä¹°å…¥é€»è¾‘"""
    threading.Thread(target=one_night_strategy.daily_buy_routine).start()
    return {"status": "triggered", "message": "Buy routine started in background."}

@app.post("/api/strategy/onenight/trigger_sell")
def trigger_onenight_sell():
    """ã€æµ‹è¯•ç”¨ã€‘æ‰‹åŠ¨è§¦å‘å–å‡ºé€»è¾‘"""
    threading.Thread(target=one_night_strategy.daily_sell_routine).start()
    return {"status": "triggered", "message": "Sell routine started in background."}


@app.get("/api/review/performance")
def get_performance_review():
    """ã€ç»©æ•ˆå¤ç›˜ã€‘è·å–æ‰€æœ‰å·²å…³é—­æˆ–æ´»è·ƒçš„æ¨èè®¢å•è¡¨ç°ï¼ˆä½¿ç”¨æ–°çš„å›æµ‹é€»è¾‘ï¼‰"""
    try:
        today_str = datetime.date.today().strftime("%Y-%m-%d")
        
        # è·å–æ‰€æœ‰OPENçš„æ¨è
        open_recs = db.get_open_recommendations()
        if not open_recs.empty:
            try:
                symbols = open_recs['symbol'].unique().tolist()
                quotes = fetcher.get_realtime_quotes(symbols)
            except Exception as e:
                print(f"[Performance] Error fetching quotes: {e}")
                quotes = pd.DataFrame()
            
            for idx, rec in open_recs.iterrows():
                try:
                    symbol = rec.get('symbol') if 'symbol' in rec.index else None
                    if not symbol:
                        continue
                    
                    matching_quote = quotes[quotes['ä»£ç '] == symbol] if not quotes.empty else pd.DataFrame()
                    if not matching_quote.empty:
                        current_price = float(matching_quote.iloc[0]['æœ€æ–°ä»·'])
                        # ä½¿ç”¨ pandas Series çš„ç´¢å¼•æ–¹å¼ï¼ˆSeries æ”¯æŒ .get() æ–¹æ³•ï¼‰
                        try:
                            entry_price = rec.get('entry_price', None)
                            if entry_price is None or (isinstance(entry_price, (int, float)) and (pd.isna(entry_price) or entry_price == 0)):
                                entry_price = rec.get('price', None)
                            if entry_price is None or (isinstance(entry_price, (int, float)) and (pd.isna(entry_price) or entry_price == 0)):
                                entry_price = current_price
                            entry_price = float(entry_price)
                        except (ValueError, TypeError, KeyError):
                            entry_price = current_price
                        
                        try:
                            entry_date = rec.get('entry_date', None)
                            if entry_date is None or (isinstance(entry_date, str) and len(entry_date) == 0):
                                entry_date = rec.get('created_at', None)
                            if entry_date is None or (isinstance(entry_date, str) and len(entry_date) == 0):
                                entry_date = today_str
                            # ç¡®ä¿æ˜¯å­—ç¬¦ä¸²
                            if not isinstance(entry_date, str):
                                entry_date = str(entry_date)
                        except (ValueError, TypeError, KeyError):
                            entry_date = today_str
                        
                        # è·å–å½“å‰actionï¼ˆé‡æ–°è¯„ä¼°ï¼‰
                        current_action = None
                        try:
                            kline_df = fetcher.get_kline_data(symbol, days=150)
                            if not kline_df.empty:
                                # é‡æ–°ç”Ÿæˆæ¨èï¼Œè·å–å½“å‰action
                                temp_rec = engine.generate_recommendation(kline_df, sentiment_score=0.0)
                                current_action = temp_rec.get('action')
                        except Exception as e:
                            print(f"[Performance] Error re-evaluating {symbol}: {e}")
                            pass
                        
                        # åˆ¤æ–­æ˜¯å¦åº”è¯¥å–å‡º
                        try:
                            # ç¡®ä¿ entry_price æ˜¯ float
                            entry_price_float = float(entry_price) if entry_price else current_price
                            # ç¡®ä¿ entry_date æ˜¯å­—ç¬¦ä¸²æ ¼å¼
                            if isinstance(entry_date, str):
                                entry_date_str = entry_date[:10] if len(entry_date) >= 10 else entry_date
                            else:
                                entry_date_str = str(entry_date)[:10] if entry_date else today_str
                            
                            exit_decision = backtest_evaluator.should_exit(
                                entry_price=entry_price_float,
                                current_price=current_price,
                                entry_date=entry_date_str,
                                current_date=today_str,
                                current_action=current_action
                            )
                        except Exception as e:
                            print(f"[Performance] Error in should_exit for {symbol}: {e}")
                            continue
                        
                        if exit_decision['should_exit']:
                            try:
                                # è®¡ç®—æœ€å¤§ç›ˆåˆ©/äºæŸï¼ˆç®€åŒ–ç‰ˆï¼Œå®é™…åº”è¯¥ä»å†å²ä»·æ ¼è®¡ç®—ï¼‰
                                max_profit = max(0, exit_decision['pnl']) if exit_decision['pnl'] > 0 else None
                                max_loss = min(0, exit_decision['pnl']) if exit_decision['pnl'] < 0 else None
                                
                                # è®¡ç®—ç›¸å¯¹å¸‚åœºæ”¶ç›Šï¼ˆç®€åŒ–ç‰ˆï¼Œå®é™…éœ€è¦å¸‚åœºæŒ‡æ•°æ•°æ®ï¼‰
                                relative_return = None  # æš‚æ—¶ä¸è®¡ç®—ï¼Œéœ€è¦å¸‚åœºæŒ‡æ•°æ•°æ®
                                
                                # è·å–è®°å½•ID
                                rec_id = rec.get('id') if 'id' in rec.index else None
                                if rec_id:
                                    # æ›´æ–°æ¨èè®°å½•
                                    db.update_recommendation(
                                        rec_id,
                                        exit_decision['exit_price'],
                                        exit_decision['pnl'],
                                        exit_reason=exit_decision['exit_reason'],
                                        max_profit=max_profit,
                                        max_loss=max_loss,
                                        relative_return=relative_return
                                    )
                            except Exception as e:
                                print(f"[Performance] Error updating recommendation for {symbol}: {e}")
                                continue
                except Exception as e:
                    print(f"[Performance] Error processing record {idx}: {e}")
                    import traceback
                    traceback.print_exc()
                    continue

        # è·å–æ‰€æœ‰å·²å…³é—­çš„æ¨è
        try:
            performance = db.get_performance_stats()
            
            # è®¡ç®—ç»©æ•ˆæŒ‡æ ‡
            if not performance.empty and len(performance) > 0:
                # ç¡®ä¿å¿…è¦çš„åˆ—å­˜åœ¨
                if 'pnl' not in performance.columns:
                    performance['pnl'] = 0.0
                if 'score' not in performance.columns:
                    performance['score'] = 0.0
                if 'hold_days' not in performance.columns:
                    performance['hold_days'] = None
                
                # å¤„ç† NaN å€¼
                performance['pnl'] = performance['pnl'].fillna(0.0)
                performance['score'] = performance['score'].fillna(0.0)
                
                try:
                    metrics = backtest_evaluator.calculate_performance_metrics(performance)
                except Exception as e:
                    print(f"[Performance] Error calculating metrics: {e}")
                    import traceback
                    traceback.print_exc()
                    metrics = {}
                
                # åªæœ‰å½“æœ‰ score åˆ—æ—¶æ‰è®¡ç®—åˆ†ç»„æŒ‡æ ‡
                grouped_metrics = {}
                if 'score' in performance.columns and not performance['score'].isna().all():
                    try:
                        grouped_metrics = backtest_evaluator.calculate_grouped_metrics(performance, group_by='score_range')
                    except Exception as e:
                        print(f"[Performance] Error calculating grouped metrics: {e}")
                        import traceback
                        traceback.print_exc()
                        grouped_metrics = {}
                
                try:
                    # æ¸…ç†NaNå€¼ï¼Œç¡®ä¿JSONå¯åºåˆ—åŒ–
                    performance_clean = performance.copy()
                    # å°†æ‰€æœ‰NaNæ›¿æ¢ä¸ºNoneï¼ˆJSONä¼šè½¬æ¢ä¸ºnullï¼‰
                    performance_clean = performance_clean.where(pd.notna(performance_clean), None)
                    
                    # è½¬æ¢ä¸ºå­—å…¸å¹¶æ¸…ç†NaN
                    performance_dict = performance_clean.to_dict(orient="records")
                    # é€’å½’æ¸…ç†å­—å…¸ä¸­çš„NaNå€¼
                    def clean_nan(obj):
                        if isinstance(obj, dict):
                            return {k: clean_nan(v) for k, v in obj.items()}
                        elif isinstance(obj, list):
                            return [clean_nan(item) for item in obj]
                        elif isinstance(obj, float) and (pd.isna(obj) or np.isnan(obj) or np.isinf(obj)):
                            return None
                        return obj
                    
                    performance_dict = clean_nan(performance_dict)
                    metrics_clean = clean_nan(metrics)
                    grouped_metrics_clean = clean_nan(grouped_metrics)
                    
                    return {
                        'performance_data': performance_dict,
                        'overall_metrics': metrics_clean,
                        'grouped_metrics': grouped_metrics_clean
                    }
                except Exception as e:
                    print(f"[Performance] Error converting to dict: {e}")
                    import traceback
                    traceback.print_exc()
                    return {
                        'performance_data': [],
                        'overall_metrics': {},
                        'grouped_metrics': {}
                    }
            else:
                return {
                    'performance_data': [],
                    'overall_metrics': {},
                    'grouped_metrics': {}
                }
        except Exception as e:
            print(f"[Performance] Error in get_performance_review: {e}")
            import traceback
            traceback.print_exc()
            return {
                'performance_data': [],
                'overall_metrics': {},
                'grouped_metrics': {},
                'error': str(e)
            }
    except Exception as e:
        print(f"[Performance] Fatal error in get_performance_review: {e}")
        import traceback
        traceback.print_exc()
        return {
            'performance_data': [],
            'overall_metrics': {},
            'grouped_metrics': {},
            'error': str(e)
        }

@app.get("/api/review/iterate")
def iterate_strategy():
    """ã€è‡ªè¿­ä»£ã€‘æ ¹æ®å†å²èƒœç‡è‡ªåŠ¨ä¼˜åŒ–æ¨¡å‹æƒé‡"""
    # 1. è·å–æ‰€æœ‰å†å²å¤ç›˜æ•°æ®
    performance = db.get_performance_stats()
    if performance.empty or len(performance) < 5:
        return {"status": "skipped", "reason": "æ ·æœ¬ä¸è¶³ï¼Œè‡³å°‘éœ€è¦5æ¡å¤ç›˜æ•°æ®æ‰èƒ½å¯åŠ¨è¿­ä»£"}
    
    # 2. è®¡ç®—èƒœç‡
    win_rate = len(performance[performance['pnl'] > 0]) / len(performance)
    avg_pnl = performance['pnl'].mean()
    
    # 3. ç®€å•çš„è¿­ä»£é€»è¾‘ (å¯å‘å¼)
    # å¦‚æœè¿‘æœŸçš„èƒœç‡ä½äº 50% ä½†æ¶ˆæ¯é¢å¾—åˆ†è¾ƒé«˜æ—¶æ”¶ç›Šå¥½ï¼Œåˆ™å¢åŠ æ¶ˆæ¯é¢æƒé‡
    weights = db.get_weights()
    current_tech = weights.get('tech_weight', 0.8)
    current_sent = weights.get('sentiment_weight', 0.2)
    
    if avg_pnl < 0:
        # å¦‚æœæœ€è¿‘äºæŸï¼Œå°è¯•å¾®è°ƒæƒé‡ï¼ˆæ¢ç´¢æ€§æ›´æ–°ï¼‰
        new_sent = min(0.4, current_sent + 0.05)
        new_tech = 1.0 - new_sent
        db.update_weight('sentiment_weight', new_sent)
        db.update_weight('tech_weight', new_tech)
        return {
            "status": "optimized",
            "win_rate": f"{win_rate*100:.2f}%",
            "new_weights": {"tech": new_tech, "sentiment": new_sent},
            "msg": "ç”±äºè¿‘æœŸèƒœç‡æ³¢åŠ¨ï¼Œå·²è‡ªåŠ¨å¢åŠ æ¶ˆæ¯é¢åé¦ˆæƒé‡ä»¥å¯¹å†²è¶‹åŠ¿æ»åæ€§ã€‚"
        }
    
    return {
        "status": "stable",
        "win_rate": f"{win_rate*100:.2f}%", 
        "weights": weights,
        "msg": "å½“å‰ç­–ç•¥è¡¨ç°ç¨³å¥ï¼Œå‚æ•°æ— éœ€è°ƒæ•´ã€‚"
    }

@app.get("/api/review/weights")
def get_current_weights():
    """è·å–å½“å‰ç­–ç•¥æƒé‡å‚æ•°"""
    return db.get_weights()

@app.post("/api/review/update_weights")
def update_manual_weights(data: dict):
    """æ‰‹åŠ¨ä¿å­˜ç­–ç•¥æƒé‡å‚æ•°"""
    try:
        tech = float(data.get('tech_weight', 0.8))
        sent = float(data.get('sentiment_weight', 0.2))
        
        # ç¡®ä¿æ€»å’Œä¸º 1 ä¸”åœ¨åˆç†åŒºé—´
        old_weights = db.get_weights()
        # ä½¿ç”¨æ›´ç¨³å¥çš„ç²¾åº¦æ¯”å¯¹ï¼Œé˜²æ­¢å‰ç«¯ä¼ å‚çš„å°æ•°ç‚¹è¯¯å·®è§¦å‘é‡æ‰«
        if abs(old_weights.get('tech_weight', 0) - tech) < 0.001 and \
           abs(old_weights.get('sentiment_weight', 0) - sent) < 0.001:
            return {"status": "success", "message": "æƒé‡å‚æ•°æœªå®è´¨å˜æ›´ï¼Œå·²è·³è¿‡å…¨é‡é‡æ‰«"}

        db.update_weight('tech_weight', tech)
        db.update_weight('sentiment_weight', sent)
        
        # è”åŠ¨ï¼šç«‹å³å¼€å¯ä¸€è½®æ–°æƒé‡çš„æ‰«æ
        scanner.trigger_scan()
        
        return {"status": "success", "new_weights": {"tech_weight": tech, "sentiment_weight": sent}, "message": "æƒé‡å·²æ›´æ–°ï¼Œå…¨é‡é‡æ‰«å·²å¯åŠ¨"}
    except Exception as e:
        return {"status": "error", "message": str(e)}

if __name__ == "__main__":
    import uvicorn
    import os
    port = int(os.getenv("PORT", 8000))
    uvicorn.run(app, host="0.0.0.0", port=port)
